
== SWIM Membership Protocol

Swift Distributed Actors uses the https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf[SWIM Membership Protocol] to determine which processes
are participating in the cluster communication and to reliably detect communication issues in a scalable manner.

=== Membership

SWIM provides a weakly consistent view on the process group membership. Membership in this context
means that we have some knowledge about the node, that was acquired by either communicating with
the node directly, for example when initially connecting to the cluster, or because some other
node shared information with us. A node can be either `alive`, `suspect`, or `dead`.
To avoid moving a node back into `alive` or `suspect` state because of older statuses that
get replicated, we need to be able put them into temporal order. For this reason each node
has an incarnation number assigned to it. This number is monotonically increasing and can only
be incremented by the respective node itself and only if it is suspected by another node in its
current incarnation. The ordering of statuses is as follows:

[[status_ordering]]
`alive(N) < suspect(N) < alive(N+1) < suspect(N+1) < dead`

A node that has been declared dead can never return from that status and has to be restarted
to join the cluster as a new node. The information about dead nodes will be kept for a configurable
amount of time, after which it will be removed to prevent the state on each node from growing
too big. The timeout value should be chosen to be big enough to prevent faulty nodes from re-joining
the cluster and is usually in the order of a few days.

=== Failure detection

Each node will periodically try to ping a random peerfootnote:swim_divergence[Our implementation diverges from the SWIM paper, <<random_ping_divergence_note,see note>>]
to determine whether that node is still reachable. If the pinged node is alive and well, it will
respond with an acknowledgement message and everything stays the same. If the pinged node does not
respond within the configured timeout, the initiating node will request a configured
number of other nodes to try and ping that node, to see if it is simply a point-to-point
communication issue, or if the node is actually unreachable. If any of the requested nodes
is able to reach the pinged node, it will forward the acknowledgement to the requesting
node and the node will remain in the alive status. If none of the nodes can reach the pinged node,
the initiating node will mark the pinged node as `suspect`. The diagram below visualizes
this process.

image::SWIM/ping_pingreq_cycle.png[SWIM Ping Cycle]

A node that is suspected to be unreachable will not immediately be marked as `dead` to reduce the
number of false positives that can be caused by temporary high load on that node, which can
cause delays in message processing, or partial network outages, where a number of nodes have
no direct connection to the pinged node. A node will be marked dead, if it can't be reached
within a configurable number of ping-cycles, which we call protocol periods. If a node can
be reached before the maximum number of protocol periods are reached, it will be marked as
`alive` again. Nodes will send the latest known status of a node together with a ping message,
to inform the node about a possible suspicion, so that it can react and increment its incarnation
number if necessary.

[[random_ping_divergence_note]]
[NOTE]
====
Our implementation diverges in the random selection of the node to ping in that it creates a
random view of all members and iterates through that list instead of chosing a fully random
node each time. This gives us the guarantee that each node is pinged once within **N** protocol
periods. New nodes will be added to the list at a random location to keep the randomness across
different nodes and not end up with newly added nodes being pinged by all nodes at the same time.
These changes have been proposed as part of the https://arxiv.org/pdf/1707.00788.pdf[Lifeguard Extensions]
to SWIM.
====

=== Gossip

SWIM uses an infection style gossip mechanism to replicate state across the cluster. The gossip
will be piggybacked onto the regular messages, i.e. whenever a node is sending a ping, a ping
request, or is responding with an acknowledgement, it will include the latest gossip with that
message as well. When a node receives gossip, it has to apply the statuses to its local state
according to the <<status_ordering,ordering stated above>>. If a node receives gossip about itself,
it has to react accordingly. If it is suspected by another node in its current incarnation, it has
to increment its incarnation in response. If it has been marked as dead, it has to shut itself down.
