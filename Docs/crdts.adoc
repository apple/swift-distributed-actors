
[[crdts]]
== Distributed Data / CRDT

> **C**onvergent **R**eplicated **D**ata **T**ypes are a category of data types that allow multiple peers to perform updates
> concurrently, without coordination, yet guarantee that after merging the independently updated types, converge at a predictable end state.

Distributed Actors provide built-in <<crdt_types, data types>>, and <<direct_replication>> as well as <<gossip_replication>>
mechanisms which simplify their use in an actor system by handling all their replication _transparently_.

CRDTs are a powerful building block for highly concurrent, distributed and--most interestingly--coordination-free systems.
Using them it is possible to build offline-first client applications (which sync with CRDTs stored in backend systems or other devices),
or coordination-free backend systems such that multiple servers can independently update data types without having to coordinate
(e.g. via consensus protocols) whether or not an update is to be accepted or not.

CRDTs are https://en.wikipedia.org/wiki/Eventual_consistency[eventually consistent] meaning that they _eventually_ converge at a known expected result, intermediate states may transiently globally inconsistent.
This is a trade-off these data structures take - being the "flip side" of strongly consistent mechanisms such as distributed consensus algorithms (which do require the participation of a quorum of peers in order to accept writes).

In other words, CRDTs are very useful in situations where eventual consistency, and perhaps offline/online and multiple-concurrent edits are to be permitted.

TIP: For consistency-first use cases please reach out as we would like to provide implementations for consensus style Replicated State Machines in the future.


=== CRDT QuickStart

TIP: QuickStarts are only a small "sneak peek" of a module.
     For an in-depth discussion of the features, refer to the following documentation sections.

==== Step 0: CRDT types and semantics

CRDTs are very useful category of data types in distributed systems. You may have already experienced them
in action first hand, perhaps without even being aware of it.

Refer to the <<crdt_types>> to understand more about the provided types and their semantics.

For this quickstart, we'll use an "Observed Removed Set" api:CRDT.ORSet[enum], which is a fairly common CRDT
and is already slightly more complex than the simplest datatype available (a Grow-only api:CRDT.GCounter[struct]).

An ORSet is able to handle additions and removals from the set, and in case a conflict is detected (i.e. multiple nodes
performed an add and remove concurrently) the _add_ operation _wins_. These semantics are useful for situations when it is
safer to err on keeping "more" information in the set than removing it.

==== Step 1: "Owning" a CRDT

In this example we'll build a _distributed shopping list_. Two actors will "own" (i.e. have a copy of) the shopping list
and either may modify it by adding and checking off items from it as they go through the store buy the necessary things and perhaps remember what else they should buy.

In our example the actors could be thought of as actual _people_ doing their shopping in a store while updating their shopping list.
As there may be many actors "sharing" the same shopping list CRDT there is a possibility that both of them perform updates to their copies of the shopping list "at the same time".
Because they are shopping in different stores, and have not discussed with each other up-front whom should buy which items from the list, there is a possibility of overlapping updates.


[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_owned_actorable]
----
<1> Rather than creating a "plain" CRDT, we create an api:ActorOwned[class] version of it,
    which will handle all distribution aspects of writes and reads to it and its distributed counterparts.

==== Step 2: Adding and "Checking off" items from the shopping list (Direct Replication)

First, we'll need to populate our shopping list with some items. The actor can expose an actor function that when received
will add given item to the list:

[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_direct_write_add]
----
<1> Similarity to a "normal" `Set`, we perform an `insert` operation on it.
<2> Since the data type is distributed, we also specify to how many nodes the delta should be pushed immediately; here we picked "local" since we're simulating the offline and sync later case.
<3> For _direct replication_ (which this operation is), we always specify an expected timeout, for when we expect all peers we write to to confirm those writes back.
<4> Finally, the `insert` returned a write result that we can use to apply a callback to inspect if all targeted peers have confirmed the write or not (most useful in `.quorum` writes).
    🟢 _This callback is safe to access internal actor state._

The remove operation follows the same pattern:

[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_direct_write_remove]
----

And similarity, we can provide `writeConsistency` which means to how many peer members in the cluster this write will be
_directly_ ("right now") replicated to. To learn more about direct replication refer to the <<direct_replication>> section.

TIP: Such shopping list based on CRDTs will _not_ guarantee that the shoppers won't accidentally double-buy some items, the writes are replicated asynchronously after all.
  The ORSet's semantics also mean, that if write conflicts were to happen, they are resolved by the "added" value winning, meaning that an item that an actor thought was removed
  could appear again if another actor added it in the meantime. In the real world such trade-off is less harmful than accidentally removing an item after all -- the user can check off the item again if needed,
  while a "remove" always winning could yield more confusing behavior. +
  +
  If we wanted to _guarantee_ that only a single shopper would buy a specific item, we would have to implement some form of coordination between them (most often via a form of consensus). +

Now that we implemented an actor which can add and check off items from its shopping list, let us spawn two actors (they could be located on different nodes) and have them independently
add and remove some items from their respective owned shopping list instances as they perform their shopping tasks.

[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_alice_bob_writes]
----

==== Step 3: Reacting to changes in the shopping list items (Gossip Replication)

While the previous step already introduced some distributed replication, it was "direct" and not in the background.
Meaning, the changes when we performed a write were replicated directly when we made the change, and specifically to at-least
as many cluster members as we specified in the `writeConsistency`. In our example we wrote only locally, but in normal
datacenter scenarios it is more usual to write to a quorum, for example like this:

[source]
----
data.insert(element, writeConsistency: .quorum, timeout: .seconds(3))
----

This is already better if we were an on-line replicated shopping list that replicates its in memory state to multiple nodes members, e.g. by using `.atLeast(2)` or `.quorum`.
However it would not handle spreading the data to members which join the cluster at a later time, nor would it spread the information to any not-directly-written to members.
This is why _actor owned_ CRDTs also automatically participate in Gossip Replication.

Gossip Replication is a transparent, running in the background, process that runs on every node participating in a cluster (and using CRDTs) which ensures
that all members (#TODO: with a given cluster role#) eventually get up-to-date with regards to the active data types.

In our example, Alice and Bob only performed `.local` writes, which means their writes would not end up on each-others systems,
unless gossip replication took them there (!). Local writes are a good way to see gossip replication do its magic - since we
did not write to any other nodes, the only way those writes end up on other members is via gossip (which we will explore in depth in <<gossip_replication>>).

In order to observe incoming writes to an api:ActorOwned[class] CRDT, we can use the `ActorOwned.onUpdate` function to install a listener,
which will be fired every time the CRDT experiences a change in its state (e.g. via direct, or gossip replication):

[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_bob_onUpdate]
----

This callback will be triggered every time the CRDT gets updated by remote (direct, or gossip) replication.
It also works the same way if you have multiple actors owning the same CRDT identity in the same cluster member,
which is useful for testing - there is no need to change your programming model at all between programming in a single process
and programming a highly distributed deployment of the same CRDT and Actor based system.

TIP: All callbacks on `ActorOwned` types are actor-concurrency-safe, meaning that they will be executed on the actor's context,
and are guaranteed to not overlap with any other actor message handling. 🟢 In other words, they are completely thread safe and may
access and modify the actor state without taking any locks or other means of synchronization.

Alternatively, there exist functions to _peek_ at an _actor owned_ CRDT's last observed value and when it was most recently updated:

[source]
----
include::{dir_sact_doc_tests}/CRDT/CRDTDocExamples.swift[tag=quickstart_peek_state]
----


[[crdt_types]]
=== Built-in CRDT Types

Distributed Actors comes with a number of pre-defined https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#State-based_CRDTs[State-Based CRDT] types (also known as _CvRDTs_), such as:

- Counters:
  * Grow-only Counter api:CRDT.GCounter[struct]
- Sets:
  * Observed Removed Set api:CRDT.ORSet[struct]
- Maps (Dictionaries):
  * Observed Removed Map api:CRDT.ORMap[struct],
  * Observed Removed MultiMap api:CRDT.ORMultiMap[struct] (a dictionary where values are of type `ORSet`),
  * ⚠️ "Last Writer Wins" Map api:CRDT.LWWMAP[struct], which is wall-clock time sensitive and should be used only very infrequently
- Registers:
  * ⚠️ "Last Writer Wins" Register api:CRDT.LWWRegister[struct], which is wall-clock time sensitive and should be used only very infrequently

[[direct_replication]]
=== Direct Replication

Direct replication is what happens when a write to an actor owned CRDT is performed using the `write(writeConsistency:timeout:)`
function, or its close cousins such as `ORSet.insert(:writeConsistency:timeout)` and similar.

==== Direct writes

Direct replication allows performing immediate writes (sends of the CRDT change/delta) to remote peers.

When a write is issued, e.g. by invoking `ownedSet.insert("apples", writeConsistency: .quorum, timeout: .seconds(3))`
the system's direct replicator is notified about this action, informs all other actors on the same node which may be owning
the CRDT identified with the same `CRDT.Identity` (which is assigned at creation time), and then proceeds to notify other nodes.

The number of nodes the replicator contacts is configured by the `writeConsistency:`api:CRDT.OperationConsistency[enum] parameter.
Refer to it's API documentation for detailed explanation of the consistency levels, however in general it simply translates to a
number of peers that will be notified about the change _and acknowlage this write back_ to the originator of the write.

Once enough (e.g. `.quorum` or `.atLeast(2)`) acknowlagements have been received by the initiating direct replicator,
it confirms the write as successful and the callback which can be assigned to the write's result is executed (🟢 `write.onComplete`).
The `onComplete` is always safely executed on the owning actor's context, so you do not need to perform any additional synchronization
and are free to modify the actor's internal state.

A common pattern includes performing a write to at least a few nodes, and replying to whichever task has kicked off this
write once the `onComplete` triggers -- thanks to this we can be sure that the piece of information has been replicated to
at least some peers already, and even if _this_ cluster member were to crash immediately after the data would remain in the cluster.

NOTE: The current implementation does not persist the replicated data types, however this is another natural extension
to the replicator in which the writes would only be confirmed once peers have stored them on disk or some other storage.
If this is a requirement for your use case, please get in touch and we can prioritize this work.

Finally, as most tasks in Distributed Actors, an operation is bound by a timeout, after which the `onComplete` is triggered
with a failure. The failure will include information about how many acknowledgements have been gathered, how many were missing (timed out),
or perhaps if a write was incorrect because the identity did not match type-wise with the rest of the system. Do note however,
that a write which was replicated to some (or even zero remote) peers, will still *eventually* spread to all nodes, thanks
to the background actor which performs periodic gossip replication of all CRDTs.

All peers in a cluster participate in the dissemination of CRDTs.
(#TODO: cluster roles can give fine grained control over this# More fine grained control is something we'd like to add, but is not implemented currently.)

==== Direct reads

Similarity, one may perform a direct read, one may issue `read(atConsistency:timeout:)` which instructs
the direct replicator to perform a read from as many peers as configured using the passed in api:CRDT.OperationConsistency[enum].

Direct reads can be useful to aggressively "refresh" the state of an actor owned CRDT, without having to wait for gossip
replication to spread any new information to our node. Such read is of course not "transactional" as one might think about
those in a traditional database, however thanks to the properties of CRDTs it has some useful benefits:

Given a 3 node cluster, of nodes `A`, `B` and `C` if node `A` performs a `.read(atConsistency: .all, timeout: .seconds(...))`,
even if nodes `B` and `C` have their own local changes made to the CRDT the read from all will automatically gather them and
_merge_ the CRDTs into one resulting value which is then passed as result to the `read`. In other words, the gathering and merging
of CRDTs is done transparently and we can configure how many nodes we want to reach with such read.

Quite often it is enough to read from just 2 or quorum of nodes, as the reads and nature of the CRDTs is eventually consistent in any case.
A `.read(.all)` however can be useful if data is always written directly only locally, and gossip is taking care of its dissemination,
however for some reason we'd like to increase the likelyhood of seeing "recent" updates.

TIP: Usually performing operations at `.quorum` is a good default, unless you have other specific needs for the write's consistency.

[[gossip_replication]]
=== Gossip Replication

WARNING: The Gossip replicator implementation is still very much work in progress and today relies on full-state sync,
  which is prohibitive for larger CRDT types. We have previous experience building gossip delta replicated systems, and the
  current types will support it natively, however the replicator today has not been implemented to benefit from these yet.
  *This is a matter of prioritization more than anything, if you have an use case for delta CRDTs in your project, please reach out.*

With https://en.wikipedia.org/wiki/Gossip_protocol[Gossip] Replication, clustered peers periodically exchange information
and synchronize state of their owned CRDTs. This is done automatically and transparently in the background for _any_
api:CRDT.ActorOwned[struct]/api:CRDT.ActorableOwned[struct] CRDTs registered in the actor system.

The system runs a special actor called the gossip replicator which is responsible for synchronizing states of the owned CRDTs.
Currently this replication is implemented in it's most naive form and has to gossip the entire payload to other peers,
when a peer notices it is not up-to date with another's state. #TODO: This is not the final implementation of gossip replication
and we will eventually implement delta-replication which the CRDT types themselfes are already prepared for.#

In general, without diving into the dissimination protocol details, gossip replication means that even a `.local` write
eventually ends up visible on _all_ cluster members. The exact timing of when it is going to be visible is not strongly guaranteed,
but generally is an function of the cluster size, and gossip interval configuration (which can be set in the system's settings).

TIP: Currently CRDTs are shared between all members of a cluster. Once cluster nodes gain "roles" it will be possible to
  control which members should participate in the replication of a specific CRDT. E.g. only "gameplay" nodes need to host
  values relating to any gameplay related things or similar. See: https://github.com/apple/swift-distributed-actors/issues/636
