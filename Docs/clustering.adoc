
== Clustering ✗

> One actor is no actor, they come in systems.

By connecting several nodes into a cluster you gain the ability to _transparently_ send and receive messages from actors
located on other nodes. Along with this, advanced failure detection and mitigation mechanisms are also enabled by default.
#TODO: well, not yet since we didn't implement them yet#

=== Joining a Cluster

In order to allow actors to message each other and make use of failure detection mechanisms, nodes on which they reside
have to form a cluster. Clustering is relatively lightweight and provides cluster membership, and a gossip based failure
detection and information dissemination protocol. #TODO: both of which are configurable and can be 'participated in',
or plugged into external systems.#

TIP: All capabilities that one has learned about actors "locally" automatically translate and work without any code-changes
     in a clustered environment.

==== Node Lifecycle

Nodes progress through a well-defined lifecycle when joining a cluster.

#TODO: the exact details are to be defined more soon, for now we do not really implement these, and only allow for "join/leave"#

image::cluster_lifecycle.svg[Cluster Node Lifecycle]

==== Joining nodes (manually)

In order for actors to be able to communicate across nodes, the nodes they reside one have to form a cluster (with exceptions,
#TODO: ServiceRefs are the exception#).

Forming a cluster is fairly simple, and consists of a few lines of configuration and joining nodes to appropriate addresses:

[source]
----
include::{dir_sact_doc_tests}/ClusterDocExamples.swift[tag=joining]
----
<1> Enable clustering (so the node binds to a local port and begins listening for connections), and optionally change which address to bind to.
<2> Join the other node, forming a 2 node cluster. The address could be discovered using some mechanism, or known up front and configured into the app. #TODO to be replaced with discovery mechanism / seed nodes#

Once the nodes have established connections, you are ready to transparently send messages across the network.

#TODO serialization first though...? or we first show that we have some built in types and THEN show serialization.#

#TODO proper intro and diagram of how nodes join#

NOTE: In some situations it may happen that a node crashes and comes up again using the same address (system name, host and port)
      and attempts to re-connect to an existing cluster. While the cluster would eventually detect the "previous incarnation"
      of this node as crashed, the restarted one _may_ be faster to attempt a join than the failure detector triggering the downing
      of the old node.

      In these situations, the "new incarnation" causes the immediate downing and removal of the previous incarnation
      and "takes its place", however from the clusters perspective, it is not any special node -- it is treated the same way as
      any other freshly joined node (and has to go through the joining steps as any other node would).

==== Node discovery (automatic)

#TODO: discovery is not implemented yet;#

It will be a pluggable mechanism that is queried by the system and should return a list of nodes that should be attempted
to join to by the current node. This allows to automatically join existing clusters etc. Assisted rollouts are also a feature
that we are planning here - e.g. for green/blue deployments and similar, and allowing coordination of these with a node
orchestrator such as k8s or similar.

=== Receptionist

Since in order to communicate with actors, one has to obtain well-typed `ActorRef<Message>` instances, it is not possible
to do so safely without consulting some form of "registry." Such registry is implemented in form of the `Receptionist`.
In cluster mode, the receptionists periodically sync their state. The interval of this sync can be configured though
the `receptionistSyncInterval` setting on api:ClusterSettings[struct]. See <<actors.adoc#receptionist,Receptionist>>
for more information.

#TODO: maybe after all we would call it 'registry', needs consideration; Receptionist is a rather known pattern for actors,
and wording is useful in the way that it keeps us in thinking about interactions with entities (as-if people).#

=== Replicated State Machines ✗

#TODO#

=== Failure detection in Clusters

Failure detection in clusters enables receiving api:Signals/Terminated[enum] signals with regards to watched actors residing on nodes,
which have entirely crashed.

Sadly, failure detection in distributed systems is always a trade-off between the time to signal a node failure,
and potential false positives (where a node is detected as failed, however it was only "very slow to respond").

==== Watching Remote Actors

Watching remote actors works the same way as it does for local actors -- if the remote actor terminates and was watched,
the watching actor gets a api:Signals/Terminated[enum] signal and may react to it.

Additionally, not only individual actor failures are reported back, but also node failures.
When a node hosting actors fails, all of the hosted actors are assumed to have terminated (upon the node's move to `Down`
lifecycle state), upon this change local watchers receive a api:Signals/Terminated[enum] signal about actors that they have watched on
given remote node. The following simplified diagram explains this mechanism:

[.center]
image::remote_watch_terminated.png[Remote Watch, when remote node crashes]

[unstyled]
- ❶ The `A` actor watches a remote actor `R` on another node,
- ❷ The node `B` crashes abruptly, without performing a graceful leave,
- ❸ The failure detector on node `A` eventually realizes what happened,
- ❹ And triggers a `Terminated()` signal for all actors on the remote (now `Down`) node, notifying our local actor `A` which had watched `R` before.

==== Custom Failure Detectors

#TODO: we will likely allow plugging your own failure detector, while we'll implement a gossip based one by default,
some users may have different needs for this or want to rely on other systems to check the health of nodes.#

=== Secure Communication using TLS

Communication between nodes can be secured with TLS by adding the necessary configuration.

[source]
----
include::{dir_sact_doc_tests}/ClusteringDocExamples.swift[tag=config_tls]
----
<1> TLSConfiguration is contained in the NIOSSL module
<2> We need to provide a path the a certificate chain
<3> And private key
<4> Certificate verification is optional and can be set to `.none` for no verification
<5> When using self signed certificates, set the trust to a certificate chain containing the certificates of all nodes that this node will be communicating with

NOTE: The server side connection will use `.noHostnameVerification`, even when `.fullVerification` is configured, as we don't know which nodes we'll be communicating with.

If the private key is protected with a passphrase, a passphrase callback has to be configured. The callback has a single parameter, that is the setter for the passphrase,
which has to be called with the passphrase encoded as `[UInt8]`.

[source]
----
include::{dir_sact_doc_tests}/ClusteringDocExamples.swift[tag=config_tls_passphrase]
----
